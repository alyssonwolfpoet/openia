# ğŸ¤– openia â€” LLM Local com Ollama + Open WebUI

Este projeto fornece um ambiente completo para rodar **modelos de linguagem (LLMs)** localmente utilizando [Ollama](https://ollama.com) e a interface grÃ¡fica [Open WebUI](https://github.com/open-webui/open-webui).

## ğŸš€ VisÃ£o geral

- ğŸ§  **Ollama**: executa modelos LLM localmente via API.
- ğŸŒ **Open WebUI**: interface web para interagir com os modelos.
- ğŸ’¾ **PersistÃªncia**: volumes Docker garantem que dados e modelos nÃ£o sejam perdidos.
- ğŸ“¦ **Modelos prÃ©-instalados**:
  - `llama3.2`
  - `nomic-embed-text`

---

## ğŸ“ Estrutura do projeto

```bash
openia/
â”œâ”€â”€ docker-compose.yml      # Define os serviÃ§os Ollama e WebUI
â”œâ”€â”€ README.md               # Este arquivo
â””â”€â”€ zera.ps1                # Script PowerShell para limpar o ambiente Docker
````

---

## ğŸ› ï¸ Como usar

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/alyssonwolfpoet/openia.git
cd openia
```

### 2. Suba os containers

```bash
docker compose up -d
```

### 3. Acesse pelo navegador

* ğŸŒ Open WebUI: [http://localhost:3000](http://localhost:3000)
* ğŸ”Œ API do Ollama: [http://localhost:11434](http://localhost:11434)

---

## ğŸ§¼ Limpeza do ambiente Docker

### Executar o script de limpeza total (recomendado para ambiente local):

```powershell
.\zera.ps1
```

### Ou, manualmente:

```powershell
# Parar todos os containers
docker ps -q | ForEach-Object { docker stop $_ }

# Remover todos os containers
docker ps -aq | ForEach-Object { docker rm $_ }

# Remover todas as imagens
docker images -q | ForEach-Object { docker rmi -f $_ }

# Remover todos os volumes
docker volume ls -q | ForEach-Object { docker volume rm $_ }

# Limpeza geral
docker system prune --all --force --volumes
```

---

## ğŸ§  Gerenciar modelos manualmente

VocÃª pode rodar comandos diretamente dentro do container `ollama`:

### Listar modelos disponÃ­veis:

```bash
docker exec -it ollama ollama list
```

### Baixar um novo modelo:

```bash
docker exec -it ollama ollama pull mistral
```

### Remover modelo:

```bash
docker exec -it ollama ollama rm llama3.2
```

---

## ğŸ Logs e Debug

### Ver logs em tempo real:

```bash
docker compose logs -f
```

### Ver logs apenas do WebUI:

```bash
docker logs -f open-webui
```

### Ver logs do Ollama:

```bash
docker logs -f ollama
```

---

## ğŸ“¦ Parar / Reiniciar containers

### Parar os serviÃ§os:

```bash
docker compose down
```

### Reiniciar os serviÃ§os:

```bash
docker compose down && docker compose up -d
```

---

## ğŸ“ LicenÃ§a

DistribuÃ­do sob a licenÃ§a [MIT](LICENSE).

---

## ğŸ™‹â€â™€ï¸ Contribuindo

Sinta-se Ã  vontade para abrir *issues*, propor melhorias ou enviar *pull requests*!

```

Se quiser, posso tambÃ©m gerar o arquivo `zera.ps1` com seu script de limpeza, ou criar `.gitignore` e outras coisas para completar o projeto. Quer que eu faÃ§a?
```
